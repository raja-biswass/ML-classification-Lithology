{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "8bd1dc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "2593067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.classification import *\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "b81e5d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VH</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Anisotropy</th>\n",
       "      <th>GLCM PC1</th>\n",
       "      <th>GLCM PC3</th>\n",
       "      <th>GLCM PC5</th>\n",
       "      <th>DEM</th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>PC6</th>\n",
       "      <th>ClassID</th>\n",
       "      <th>Class_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-26.148289</td>\n",
       "      <td>0.802243</td>\n",
       "      <td>0.511247</td>\n",
       "      <td>-11.121046</td>\n",
       "      <td>-1.187986</td>\n",
       "      <td>-0.217070</td>\n",
       "      <td>46</td>\n",
       "      <td>1.045</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.089072</td>\n",
       "      <td>0.028547</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>1</td>\n",
       "      <td>ASK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-27.497355</td>\n",
       "      <td>0.783197</td>\n",
       "      <td>0.534009</td>\n",
       "      <td>-11.905989</td>\n",
       "      <td>0.034768</td>\n",
       "      <td>-0.557473</td>\n",
       "      <td>54</td>\n",
       "      <td>1.049</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.090146</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>0.008729</td>\n",
       "      <td>1</td>\n",
       "      <td>ASK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-22.360802</td>\n",
       "      <td>0.771204</td>\n",
       "      <td>0.547740</td>\n",
       "      <td>-7.164552</td>\n",
       "      <td>0.445399</td>\n",
       "      <td>-0.423039</td>\n",
       "      <td>54</td>\n",
       "      <td>1.027</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.087076</td>\n",
       "      <td>0.028418</td>\n",
       "      <td>0.005288</td>\n",
       "      <td>1</td>\n",
       "      <td>ASK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-20.822491</td>\n",
       "      <td>0.814251</td>\n",
       "      <td>0.496232</td>\n",
       "      <td>-5.439240</td>\n",
       "      <td>-1.435103</td>\n",
       "      <td>-0.324791</td>\n",
       "      <td>59</td>\n",
       "      <td>1.005</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.085156</td>\n",
       "      <td>0.031047</td>\n",
       "      <td>0.008023</td>\n",
       "      <td>1</td>\n",
       "      <td>ASK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-19.735399</td>\n",
       "      <td>0.846771</td>\n",
       "      <td>0.452534</td>\n",
       "      <td>-4.033033</td>\n",
       "      <td>2.218574</td>\n",
       "      <td>-0.247315</td>\n",
       "      <td>53</td>\n",
       "      <td>1.020</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.082730</td>\n",
       "      <td>0.032016</td>\n",
       "      <td>0.004915</td>\n",
       "      <td>1</td>\n",
       "      <td>ASK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          VH   Entropy  Anisotropy   GLCM PC1  GLCM PC3  GLCM PC5  DEM    PC1  \\\n",
       "0 -26.148289  0.802243    0.511247 -11.121046 -1.187986 -0.217070   46  1.045   \n",
       "1 -27.497355  0.783197    0.534009 -11.905989  0.034768 -0.557473   54  1.049   \n",
       "2 -22.360802  0.771204    0.547740  -7.164552  0.445399 -0.423039   54  1.027   \n",
       "3 -20.822491  0.814251    0.496232  -5.439240 -1.435103 -0.324791   59  1.005   \n",
       "4 -19.735399  0.846771    0.452534  -4.033033  2.218574 -0.247315   53  1.020   \n",
       "\n",
       "     PC2       PC3       PC4       PC6  ClassID Class_name  \n",
       "0  0.114  0.089072  0.028547  0.008142        1        ASK  \n",
       "1  0.117  0.090146  0.029851  0.008729        1        ASK  \n",
       "2  0.126  0.087076  0.028418  0.005288        1        ASK  \n",
       "3  0.115  0.085156  0.031047  0.008023        1        ASK  \n",
       "4  0.122  0.082730  0.032016  0.004915        1        ASK  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data = pd.read_csv(\"Classifi_afterFI2.csv\")\n",
    "my_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "454e1068",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.drop(['ClassID'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "19efe23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.drop(['VH'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "3b8c5617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Anisotropy</th>\n",
       "      <th>GLCM PC1</th>\n",
       "      <th>GLCM PC3</th>\n",
       "      <th>GLCM PC5</th>\n",
       "      <th>DEM</th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>PC6</th>\n",
       "      <th>Class_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.802243</td>\n",
       "      <td>0.511247</td>\n",
       "      <td>-11.121046</td>\n",
       "      <td>-1.187986</td>\n",
       "      <td>-0.217070</td>\n",
       "      <td>46</td>\n",
       "      <td>1.045</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.089072</td>\n",
       "      <td>0.028547</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>ASK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.783197</td>\n",
       "      <td>0.534009</td>\n",
       "      <td>-11.905989</td>\n",
       "      <td>0.034768</td>\n",
       "      <td>-0.557473</td>\n",
       "      <td>54</td>\n",
       "      <td>1.049</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.090146</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>0.008729</td>\n",
       "      <td>ASK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.771204</td>\n",
       "      <td>0.547740</td>\n",
       "      <td>-7.164552</td>\n",
       "      <td>0.445399</td>\n",
       "      <td>-0.423039</td>\n",
       "      <td>54</td>\n",
       "      <td>1.027</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.087076</td>\n",
       "      <td>0.028418</td>\n",
       "      <td>0.005288</td>\n",
       "      <td>ASK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.814251</td>\n",
       "      <td>0.496232</td>\n",
       "      <td>-5.439240</td>\n",
       "      <td>-1.435103</td>\n",
       "      <td>-0.324791</td>\n",
       "      <td>59</td>\n",
       "      <td>1.005</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.085156</td>\n",
       "      <td>0.031047</td>\n",
       "      <td>0.008023</td>\n",
       "      <td>ASK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.846771</td>\n",
       "      <td>0.452534</td>\n",
       "      <td>-4.033033</td>\n",
       "      <td>2.218574</td>\n",
       "      <td>-0.247315</td>\n",
       "      <td>53</td>\n",
       "      <td>1.020</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.082730</td>\n",
       "      <td>0.032016</td>\n",
       "      <td>0.004915</td>\n",
       "      <td>ASK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Entropy  Anisotropy   GLCM PC1  GLCM PC3  GLCM PC5  DEM    PC1    PC2  \\\n",
       "0  0.802243    0.511247 -11.121046 -1.187986 -0.217070   46  1.045  0.114   \n",
       "1  0.783197    0.534009 -11.905989  0.034768 -0.557473   54  1.049  0.117   \n",
       "2  0.771204    0.547740  -7.164552  0.445399 -0.423039   54  1.027  0.126   \n",
       "3  0.814251    0.496232  -5.439240 -1.435103 -0.324791   59  1.005  0.115   \n",
       "4  0.846771    0.452534  -4.033033  2.218574 -0.247315   53  1.020  0.122   \n",
       "\n",
       "        PC3       PC4       PC6 Class_name  \n",
       "0  0.089072  0.028547  0.008142        ASK  \n",
       "1  0.090146  0.029851  0.008729        ASK  \n",
       "2  0.087076  0.028418  0.005288        ASK  \n",
       "3  0.085156  0.031047  0.008023        ASK  \n",
       "4  0.082730  0.032016  0.004915        ASK  "
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "eba0e0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "aaefed99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ASK\n",
       "1    ASK\n",
       "2    ASK\n",
       "3    ASK\n",
       "4    ASK\n",
       "Name: Class_name, dtype: object"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=my_data.iloc[:,11]\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "05d41ead",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Anisotropy</th>\n",
       "      <th>GLCM PC1</th>\n",
       "      <th>GLCM PC3</th>\n",
       "      <th>GLCM PC5</th>\n",
       "      <th>DEM</th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>PC6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.802243</td>\n",
       "      <td>0.511247</td>\n",
       "      <td>-11.121046</td>\n",
       "      <td>-1.187986</td>\n",
       "      <td>-0.217070</td>\n",
       "      <td>46</td>\n",
       "      <td>1.045</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.089072</td>\n",
       "      <td>0.028547</td>\n",
       "      <td>0.008142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.783197</td>\n",
       "      <td>0.534009</td>\n",
       "      <td>-11.905989</td>\n",
       "      <td>0.034768</td>\n",
       "      <td>-0.557473</td>\n",
       "      <td>54</td>\n",
       "      <td>1.049</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.090146</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>0.008729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.771204</td>\n",
       "      <td>0.547740</td>\n",
       "      <td>-7.164552</td>\n",
       "      <td>0.445399</td>\n",
       "      <td>-0.423039</td>\n",
       "      <td>54</td>\n",
       "      <td>1.027</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.087076</td>\n",
       "      <td>0.028418</td>\n",
       "      <td>0.005288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.814251</td>\n",
       "      <td>0.496232</td>\n",
       "      <td>-5.439240</td>\n",
       "      <td>-1.435103</td>\n",
       "      <td>-0.324791</td>\n",
       "      <td>59</td>\n",
       "      <td>1.005</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.085156</td>\n",
       "      <td>0.031047</td>\n",
       "      <td>0.008023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.846771</td>\n",
       "      <td>0.452534</td>\n",
       "      <td>-4.033033</td>\n",
       "      <td>2.218574</td>\n",
       "      <td>-0.247315</td>\n",
       "      <td>53</td>\n",
       "      <td>1.020</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.082730</td>\n",
       "      <td>0.032016</td>\n",
       "      <td>0.004915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Entropy  Anisotropy   GLCM PC1  GLCM PC3  GLCM PC5  DEM    PC1    PC2  \\\n",
       "0  0.802243    0.511247 -11.121046 -1.187986 -0.217070   46  1.045  0.114   \n",
       "1  0.783197    0.534009 -11.905989  0.034768 -0.557473   54  1.049  0.117   \n",
       "2  0.771204    0.547740  -7.164552  0.445399 -0.423039   54  1.027  0.126   \n",
       "3  0.814251    0.496232  -5.439240 -1.435103 -0.324791   59  1.005  0.115   \n",
       "4  0.846771    0.452534  -4.033033  2.218574 -0.247315   53  1.020  0.122   \n",
       "\n",
       "        PC3       PC4       PC6  \n",
       "0  0.089072  0.028547  0.008142  \n",
       "1  0.090146  0.029851  0.008729  \n",
       "2  0.087076  0.028418  0.005288  \n",
       "3  0.085156  0.031047  0.008023  \n",
       "4  0.082730  0.032016  0.004915  "
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=my_data.iloc[:,0:11]\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "c28dcf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "fec61e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay, \\\n",
    "                            precision_score, recall_score, f1_score, roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "429a9c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model performance for training set\n",
      "Train Accuracy: 0.877373417721519\n",
      "Train_F1 score: 0.8749141550890425\n",
      "Train_Recall: 0.877373417721519\n",
      "Train_Precision Score: 0.877373417721519\n",
      "............................................\n",
      "model performance for test dataset\n",
      "Test Accuracy: 0.7462866318747491\n",
      "Test_F1 score: 0.7418772741346034\n",
      "Test_Recall: 0.7462866318747491\n",
      "Precision Score: 0.7462866318747491\n"
     ]
    }
   ],
   "source": [
    "# 1 lightgbm\n",
    "\n",
    "import lightgbm\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lightgbm = LGBMClassifier(max_depth=3)\n",
    "\n",
    "lightgbm.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "y_pred_test = lightgbm.predict(x_test)\n",
    "y_pred_train = lightgbm.predict(x_train)\n",
    "\n",
    "# Calculate and store accuracy scores\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    " # Calculate and store F1 scores\n",
    "train_f1 = f1_score(y_train, y_pred_train, average='weighted') \n",
    "test_f1 = f1_score(y_test, y_pred_test, average='weighted') \n",
    "\n",
    "# Calculate and store recall scores\n",
    "train_recall = recall_score(y_train, y_pred_train, average='weighted')  # Use 'weighted' for multiclass problems\n",
    "test_recall = recall_score(y_test, y_pred_test, average='weighted')      # Use 'weighted' for multiclass problems\n",
    "\n",
    "# Calculate and store Precision scores\n",
    "train_precision = precision_score(y_train, y_pred_train, average='weighted') \n",
    "test_precision = precision_score(y_test, y_pred_test, average='weighted') \n",
    "\n",
    "\n",
    "print('model performance for training set')\n",
    "\n",
    "print(\"Train Accuracy: \"+str(accuracy_score(y_train, y_pred_train)))\n",
    "print ('Train_F1 score:', (f1_score(y_train, y_pred_train, average='weighted')))\n",
    "print ('Train_Recall:', (recall_score(y_train, y_pred_train, average='weighted')))\n",
    "print('Train_Precision Score:',(precision_score(y_train,  y_pred_train, pos_label='positive', average='micro')))\n",
    "print('............................................')\n",
    "\n",
    "\n",
    "print('model performance for test dataset')\n",
    "\n",
    "print(\"Test Accuracy: \"+str(accuracy_score(y_test, y_pred_test)))\n",
    "print ('Test_F1 score:', (f1_score(y_test, y_pred_test, average='weighted')))\n",
    "print ('Test_Recall:', (recall_score(y_test, y_pred_test, average='weighted')))\n",
    "print('Precision Score:',(precision_score(y_test,  y_pred_test, pos_label='positive', average='micro')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "dddb2d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model performance for training set\n",
      "Train Accuracy: 0.7100474683544303\n",
      "Train_F1 score: 0.7003148892474605\n",
      "Train_Recall: 0.7100474683544303\n",
      "Train_Precision Score: 0.7100474683544303\n",
      "............................................\n",
      "model performance for test dataset\n",
      "Test Accuracy: 0.6708149337615416\n",
      "Test_F1 score: 0.6600876468690361\n",
      "Test_Recall: 0.6708149337615416\n",
      "Test_Precision Score: 0.6708149337615416\n"
     ]
    }
   ],
   "source": [
    "# 2 xgboost\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.fit_transform(y_test)\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "xgb = XGBClassifier(max_depth=1)\n",
    "\n",
    "xgb.fit(x_train, y_train)\n",
    "\n",
    "y_pred_test = xgb.predict(x_test)\n",
    "y_pred_train = xgb.predict(x_train)\n",
    "\n",
    "# Calculate and store accuracy scores\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    " # Calculate and store F1 scores\n",
    "train_f1 = f1_score(y_train, y_pred_train, average='weighted') \n",
    "test_f1 = f1_score(y_test, y_pred_test, average='weighted') \n",
    "\n",
    "# Calculate and store recall scores\n",
    "train_recall = recall_score(y_train, y_pred_train, average='weighted')  # Use 'weighted' for multiclass problems\n",
    "test_recall = recall_score(y_test, y_pred_test, average='weighted')      # Use 'weighted' for multiclass problems\n",
    "\n",
    "\n",
    "# Calculate and store Precision scores\n",
    "train_precision = precision_score(y_train, y_pred_train, average='weighted') \n",
    "test_precision = precision_score(y_test, y_pred_test, average='weighted') \n",
    "\n",
    "\n",
    "print('model performance for training set')\n",
    "\n",
    "print(\"Train Accuracy: \"+str(accuracy_score(y_train, y_pred_train)))\n",
    "print ('Train_F1 score:', (f1_score(y_train, y_pred_train, average='weighted')))\n",
    "print ('Train_Recall:', (recall_score(y_train, y_pred_train, average='weighted')))\n",
    "print('Train_Precision Score:',(precision_score(y_train,  y_pred_train, pos_label='positive', average='micro')))\n",
    "\n",
    "print('............................................')\n",
    "\n",
    "\n",
    "print('model performance for test dataset')\n",
    "\n",
    "print(\"Test Accuracy: \"+str(accuracy_score(y_test, y_pred_test)))\n",
    "print ('Test_F1 score:', (f1_score(y_test, y_pred_test, average='weighted')))\n",
    "print ('Test_Recall:', (recall_score(y_test, y_pred_test, average='weighted')))\n",
    "print('Test_Precision Score:',(precision_score(y_test,  y_pred_test, pos_label='positive', average='micro')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "8a3617e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model performance for training set\n",
      "Train Accuracy: 0.6942246835443038\n",
      "Train_F1 score: 0.6769323115530458\n",
      "Train_Recall: 0.6942246835443038\n",
      "Train_Precision Score: 0.6942246835443038\n",
      "............................................\n",
      "model performance for test dataset\n",
      "Test Accuracy: 0.6415094339622641\n",
      "Test_F1 score: 0.610220245027944\n",
      "Test_Recall: 0.6415094339622641\n",
      "Test_Precision Score: 0.6415094339622641\n"
     ]
    }
   ],
   "source": [
    "#3 ExtraTree\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "ET = ExtraTreesClassifier(n_estimators=50, random_state=0, max_depth=8)\n",
    "\n",
    "# Initialize dictionaries to store scores for each model\n",
    "train_scores = {}\n",
    "test_scores = {}\n",
    "\n",
    "ET.fit(x_train, y_train)\n",
    "\n",
    "y_pred_test = ET.predict(x_test)\n",
    "y_pred_train = ET.predict(x_train)\n",
    "\n",
    " # Calculate and store accuracy scores\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    " # Calculate and store F1 scores\n",
    "train_f1 = f1_score(y_train, y_pred_train, average='weighted') \n",
    "test_f1 = f1_score(y_test, y_pred_test, average='weighted') \n",
    "\n",
    "# Calculate and store recall scores\n",
    "train_recall = recall_score(y_train, y_pred_train, average='weighted')  # Use 'weighted' for multiclass problems\n",
    "test_recall = recall_score(y_test, y_pred_test, average='weighted')      # Use 'weighted' for multiclass problems\n",
    "\n",
    "# Calculate and store Precision scores\n",
    "train_precision = precision_score(y_train, y_pred_train, average='weighted') \n",
    "test_precision = precision_score(y_test, y_pred_test, average='weighted') \n",
    "\n",
    "\n",
    "print('model performance for training set')\n",
    "\n",
    "print(\"Train Accuracy: \"+str(accuracy_score(y_train, y_pred_train)))\n",
    "print ('Train_F1 score:', (f1_score(y_train, y_pred_train, average='weighted')))\n",
    "print ('Train_Recall:', (recall_score(y_train, y_pred_train, average='weighted')))\n",
    "print('Train_Precision Score:',(precision_score(y_train,  y_pred_train, pos_label='positive', average='micro')))\n",
    "\n",
    "print('............................................')\n",
    "\n",
    "\n",
    "print('model performance for test dataset')\n",
    "\n",
    "print(\"Test Accuracy: \"+str(accuracy_score(y_test, y_pred_test)))\n",
    "print ('Test_F1 score:', (f1_score(y_test, y_pred_test, average='weighted')))\n",
    "print ('Test_Recall:', (recall_score(y_test, y_pred_test, average='weighted')))\n",
    "print('Test_Precision Score:',(precision_score(y_test,  y_pred_test, pos_label='positive', average='micro')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "dc8c7303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model performance for training set\n",
      "Train Accuracy: 0.7772943037974683\n",
      "Train_F1 score: 0.7714142335722962\n",
      "Train_Recall: 0.7772943037974683\n",
      "Train_Precision Score: 0.7772943037974683\n",
      "............................................\n",
      "model performance for test dataset\n",
      "Test Accuracy: 0.7129666800481734\n",
      "Test_F1 score: 0.703536837264327\n",
      "Test_Recall: 0.7129666800481734\n",
      "Test_Precision Score: 0.7129666800481734\n"
     ]
    }
   ],
   "source": [
    "#4 RandomForest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RF = RandomForestClassifier(random_state=0, max_depth=7)\n",
    "\n",
    "RF.fit(x_train, y_train)\n",
    "\n",
    "y_pred_test = RF.predict(x_test)\n",
    "y_pred_train = RF.predict(x_train)\n",
    "\n",
    " # Calculate and store accuracy scores\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    " # Calculate and store F1 scores\n",
    "train_f1 = f1_score(y_train, y_pred_train, average='weighted') \n",
    "test_f1 = f1_score(y_test, y_pred_test, average='weighted') \n",
    "\n",
    "# Calculate and store recall scores\n",
    "train_recall = recall_score(y_train, y_pred_train, average='weighted')  # Use 'weighted' for multiclass problems\n",
    "test_recall = recall_score(y_test, y_pred_test, average='weighted')      # Use 'weighted' for multiclass problems\n",
    "\n",
    "# Calculate and store Precision scores\n",
    "train_precision = precision_score(y_train, y_pred_train, average='weighted') \n",
    "test_precision = precision_score(y_test, y_pred_test, average='weighted') \n",
    "\n",
    "print('model performance for training set')\n",
    "\n",
    "print(\"Train Accuracy: \"+str(accuracy_score(y_train, y_pred_train)))\n",
    "print ('Train_F1 score:', (f1_score(y_train, y_pred_train, average='weighted')))\n",
    "print ('Train_Recall:', (recall_score(y_train, y_pred_train, average='weighted')))\n",
    "print('Train_Precision Score:',(precision_score(y_train,  y_pred_train, pos_label='positive', average='micro')))\n",
    "\n",
    "print('............................................')\n",
    "\n",
    "\n",
    "print('model performance for test dataset')\n",
    "\n",
    "print(\"Test Accuracy: \"+str(accuracy_score(y_test, y_pred_test)))\n",
    "print ('Test_F1 score:', (f1_score(y_test, y_pred_test, average='weighted')))\n",
    "print ('Test_Recall:', (recall_score(y_test, y_pred_test, average='weighted')))\n",
    "print('Test_Precision Score:',(precision_score(y_test,  y_pred_test, pos_label='positive', average='micro')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "75a6b10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model performance for training set\n",
      "Train Accuracy: 0.6770174050632911\n",
      "Train_F1 score: 0.6646242754002413\n",
      "Train_Recall: 0.6770174050632911\n",
      "Train_Precision Score: 0.6770174050632911\n",
      "............................................\n",
      "model performance for test dataset\n",
      "Test Accuracy: 0.639502207948615\n",
      "Test_F1 score: 0.6243404695993819\n",
      "Test_Recall: 0.639502207948615\n",
      "Test_Precision Score: 0.639502207948615\n"
     ]
    }
   ],
   "source": [
    "#5 GradientBoosting\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "GBC = GradientBoostingClassifier(max_depth=1)\n",
    "\n",
    "GBC.fit(x_train, y_train)\n",
    "\n",
    "y_pred_test = GBC.predict(x_test)\n",
    "y_pred_train = GBC.predict(x_train)\n",
    "\n",
    "# Calculate and store accuracy scores\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    " # Calculate and store F1 scores\n",
    "train_f1 = f1_score(y_train, y_pred_train, average='weighted') \n",
    "test_f1 = f1_score(y_test, y_pred_test, average='weighted') \n",
    "\n",
    "# Calculate and store recall scores\n",
    "train_recall = recall_score(y_train, y_pred_train, average='weighted')  # Use 'weighted' for multiclass problems\n",
    "test_recall = recall_score(y_test, y_pred_test, average='weighted')      # Use 'weighted' for multiclass problems\n",
    "\n",
    "# Calculate and store Precision scores\n",
    "train_precision = precision_score(y_train, y_pred_train, average='weighted') \n",
    "test_precision = precision_score(y_test, y_pred_test, average='weighted') \n",
    "\n",
    "\n",
    "print('model performance for training set')\n",
    "\n",
    "print(\"Train Accuracy: \"+str(accuracy_score(y_train, y_pred_train)))\n",
    "print ('Train_F1 score:', (f1_score(y_train, y_pred_train, average='weighted')))\n",
    "print ('Train_Recall:', (recall_score(y_train, y_pred_train, average='weighted')))\n",
    "print('Train_Precision Score:',(precision_score(y_train,  y_pred_train, pos_label='positive', average='micro')))\n",
    "\n",
    "print('............................................')\n",
    "\n",
    "\n",
    "print('model performance for test dataset')\n",
    "\n",
    "print(\"Test Accuracy: \"+str(accuracy_score(y_test, y_pred_test)))\n",
    "print ('Test_F1 score:', (f1_score(y_test, y_pred_test, average='weighted')))\n",
    "print ('Test_Recall:', (recall_score(y_test, y_pred_test, average='weighted')))\n",
    "print('Test_Precision Score:',(precision_score(y_test,  y_pred_test, pos_label='positive', average='micro')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "9816475e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model performance for training set\n",
      "Train Accuracy: 0.7114319620253164\n",
      "Train_F1 score: 0.7119497252593839\n",
      "Train_Recall: 0.7114319620253164\n",
      "Train_Precision Score: 0.7114319620253164\n",
      "............................................\n",
      "model performance for test dataset\n",
      "Test Accuracy: 0.618627057406664\n",
      "Test_F1 score: 0.6187159850956055\n",
      "Test_Recall: 0.618627057406664\n",
      "Test_Precision Score: 0.618627057406664\n"
     ]
    }
   ],
   "source": [
    "#6 DecisionTree\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "DT = DecisionTreeClassifier(max_depth=8)\n",
    "\n",
    "DT.fit(x_train, y_train)\n",
    "\n",
    "y_pred_test = DT.predict(x_test)\n",
    "y_pred_train = DT.predict(x_train)\n",
    "\n",
    "# Calculate and store accuracy scores\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    " # Calculate and store F1 scores\n",
    "train_f1 = f1_score(y_train, y_pred_train, average='weighted') \n",
    "test_f1 = f1_score(y_test, y_pred_test, average='weighted') \n",
    "\n",
    "# Calculate and store recall scores\n",
    "train_recall = recall_score(y_train, y_pred_train, average='weighted')  # Use 'weighted' for multiclass problems\n",
    "test_recall = recall_score(y_test, y_pred_test, average='weighted')      # Use 'weighted' for multiclass problems\n",
    "\n",
    "# Calculate and store Precision scores\n",
    "train_precision = precision_score(y_train, y_pred_train, average='weighted') \n",
    "test_precision = precision_score(y_test, y_pred_test, average='weighted') \n",
    "\n",
    "print('model performance for training set')\n",
    "\n",
    "print(\"Train Accuracy: \"+str(accuracy_score(y_train, y_pred_train)))\n",
    "print ('Train_F1 score:', (f1_score(y_train, y_pred_train, average='weighted')))\n",
    "print ('Train_Recall:', (recall_score(y_train, y_pred_train, average='weighted')))\n",
    "print('Train_Precision Score:',(precision_score(y_train,  y_pred_train, pos_label='positive', average='micro')))\n",
    "\n",
    "print('............................................')\n",
    "\n",
    "\n",
    "print('model performance for test dataset')\n",
    "\n",
    "print(\"Test Accuracy: \"+str(accuracy_score(y_test, y_pred_test)))\n",
    "print ('Test_F1 score:', (f1_score(y_test, y_pred_test, average='weighted')))\n",
    "print ('Test_Recall:', (recall_score(y_test, y_pred_test, average='weighted')))\n",
    "print('Test_Precision Score:',(precision_score(y_test,  y_pred_test, pos_label='positive', average='micro')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "84a765a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model performance for training set\n",
      "Train Accuracy: 0.5729825949367089\n",
      "Train_F1 score: 0.5424200624731325\n",
      "Train_Recall: 0.5729825949367089\n",
      "Train_Precision Score: 0.5729825949367089\n",
      "............................................\n",
      "model performance for test dataset\n",
      "Test Accuracy: 0.5620232838217584\n",
      "Test_F1 score: 0.5360966963560638\n",
      "Test_Recall: 0.5620232838217584\n",
      "Test_Precision Score: 0.5620232838217584\n"
     ]
    }
   ],
   "source": [
    "#7 QuadraticDiscriminantAnalysis\n",
    "\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "QDA = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "QDA.fit(x_train, y_train)\n",
    "\n",
    "y_pred_test = QDA.predict(x_test)\n",
    "y_pred_train = QDA.predict(x_train)\n",
    "\n",
    "\n",
    "# Calculate and store accuracy scores\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    " # Calculate and store F1 scores\n",
    "train_f1 = f1_score(y_train, y_pred_train, average='weighted') \n",
    "test_f1 = f1_score(y_test, y_pred_test, average='weighted') \n",
    "\n",
    "# Calculate and store recall scores\n",
    "train_recall = recall_score(y_train, y_pred_train, average='weighted')  # Use 'weighted' for multiclass problems\n",
    "test_recall = recall_score(y_test, y_pred_test, average='weighted')      # Use 'weighted' for multiclass problems\n",
    "\n",
    "# Calculate and store Precision scores\n",
    "train_precision = precision_score(y_train, y_pred_train, average='weighted') \n",
    "test_precision = precision_score(y_test, y_pred_test, average='weighted') \n",
    "\n",
    "\n",
    "print('model performance for training set')\n",
    "\n",
    "print(\"Train Accuracy: \"+str(accuracy_score(y_train, y_pred_train)))\n",
    "print ('Train_F1 score:', (f1_score(y_train, y_pred_train, average='weighted')))\n",
    "print ('Train_Recall:', (recall_score(y_train, y_pred_train, average='weighted')))\n",
    "print('Train_Precision Score:',(precision_score(y_train,  y_pred_train, pos_label='positive', average='micro')))\n",
    "\n",
    "print('............................................')\n",
    "\n",
    "\n",
    "print('model performance for test dataset')\n",
    "\n",
    "print(\"Test Accuracy: \"+str(accuracy_score(y_test, y_pred_test)))\n",
    "print ('Test_F1 score:', (f1_score(y_test, y_pred_test, average='weighted')))\n",
    "print ('Test_Recall:', (recall_score(y_test, y_pred_test, average='weighted')))\n",
    "print('Test_Precision Score:',(precision_score(y_test,  y_pred_test, pos_label='positive', average='micro')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "be54e3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model performance for training set\n",
      "Train Accuracy: 0.5727848101265823\n",
      "Train_F1 score: 0.5590923712748804\n",
      "Train_Recall: 0.5727848101265823\n",
      "Train_Precision Score: 0.5727848101265823\n",
      "............................................\n",
      "model performance for test dataset\n",
      "Test Accuracy: 0.5949417904456041\n",
      "Test_F1 score: 0.583799073493705\n",
      "Test_Recall: 0.5949417904456041\n",
      "Test_Precision Score: 0.5949417904456041\n"
     ]
    }
   ],
   "source": [
    "#8 LinearDiscriminantAnalysis\n",
    "\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "LDA = LinearDiscriminantAnalysis()\n",
    "\n",
    "LDA.fit(x_train, y_train)\n",
    "\n",
    "y_pred_test = LDA.predict(x_test)\n",
    "y_pred_train = LDA.predict(x_train)\n",
    "\n",
    "# Calculate and store accuracy scores\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    " # Calculate and store F1 scores\n",
    "train_f1 = f1_score(y_train, y_pred_train, average='weighted') \n",
    "test_f1 = f1_score(y_test, y_pred_test, average='weighted') \n",
    "\n",
    "# Calculate and store recall scores\n",
    "train_recall = recall_score(y_train, y_pred_train, average='weighted')  # Use 'weighted' for multiclass problems\n",
    "test_recall = recall_score(y_test, y_pred_test, average='weighted')      # Use 'weighted' for multiclass problems\n",
    "\n",
    "# Calculate and store Precision scores\n",
    "train_precision = precision_score(y_train, y_pred_train, average='weighted') \n",
    "test_precision = precision_score(y_test, y_pred_test, average='weighted') \n",
    "\n",
    "\n",
    "print('model performance for training set')\n",
    "\n",
    "print(\"Train Accuracy: \"+str(accuracy_score(y_train, y_pred_train)))\n",
    "print ('Train_F1 score:', (f1_score(y_train, y_pred_train, average='weighted')))\n",
    "print ('Train_Recall:', (recall_score(y_train, y_pred_train, average='weighted')))\n",
    "print('Train_Precision Score:',(precision_score(y_train,  y_pred_train, pos_label='positive', average='micro')))\n",
    "\n",
    "print('............................................')\n",
    "\n",
    "\n",
    "print('model performance for test dataset')\n",
    "\n",
    "print(\"Test Accuracy: \"+str(accuracy_score(y_test, y_pred_test)))\n",
    "print ('Test_F1 score:', (f1_score(y_test, y_pred_test, average='weighted')))\n",
    "print ('Test_Recall:', (recall_score(y_test, y_pred_test, average='weighted')))\n",
    "print('Test_Precision Score:',(precision_score(y_test,  y_pred_test, pos_label='positive', average='micro')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "fc5cccba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model performance for training set\n",
      "Train Accuracy: 0.5727848101265823\n",
      "Train_F1 score: 0.5590923712748804\n",
      "Train_Recall: 0.5727848101265823\n",
      "Train_Precision Score: 0.5727848101265823\n",
      "............................................\n",
      "model performance for test dataset\n",
      "Test Accuracy: 0.5949417904456041\n",
      "Test_F1 score: 0.583799073493705\n",
      "Test_Recall: 0.5949417904456041\n",
      "Test_Precision Score: 0.5949417904456041\n"
     ]
    }
   ],
   "source": [
    "#9 Naive Bayes\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "NB = GaussianNB()\n",
    "\n",
    "LDA.fit(x_train, y_train)\n",
    "\n",
    "y_pred_test = LDA.predict(x_test)\n",
    "y_pred_train = LDA.predict(x_train)\n",
    "\n",
    "# Calculate and store accuracy scores\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    " # Calculate and store F1 scores\n",
    "train_f1 = f1_score(y_train, y_pred_train, average='weighted') \n",
    "test_f1 = f1_score(y_test, y_pred_test, average='weighted') \n",
    "\n",
    "# Calculate and store recall scores\n",
    "train_recall = recall_score(y_train, y_pred_train, average='weighted')  # Use 'weighted' for multiclass problems\n",
    "test_recall = recall_score(y_test, y_pred_test, average='weighted')      # Use 'weighted' for multiclass problems\n",
    "\n",
    "# Calculate and store Precision scores\n",
    "train_precision = precision_score(y_train, y_pred_train, average='weighted') \n",
    "test_precision = precision_score(y_test, y_pred_test, average='weighted') \n",
    "\n",
    "print('model performance for training set')\n",
    "\n",
    "print(\"Train Accuracy: \"+str(accuracy_score(y_train, y_pred_train)))\n",
    "print ('Train_F1 score:', (f1_score(y_train, y_pred_train, average='weighted')))\n",
    "print ('Train_Recall:', (recall_score(y_train, y_pred_train, average='weighted')))\n",
    "print('Train_Precision Score:',(precision_score(y_train,  y_pred_train, pos_label='positive', average='micro')))\n",
    "\n",
    "print('............................................')\n",
    "\n",
    "\n",
    "print('model performance for test dataset')\n",
    "\n",
    "print(\"Test Accuracy: \"+str(accuracy_score(y_test, y_pred_test)))\n",
    "print ('Test_F1 score:', (f1_score(y_test, y_pred_test, average='weighted')))\n",
    "print ('Test_Recall:', (recall_score(y_test, y_pred_test, average='weighted')))\n",
    "print('Test_Precision Score:',(precision_score(y_test,  y_pred_test, pos_label='positive', average='micro')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "796a945e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model performance for training set\n",
      "Train Accuracy: 0.5104825949367089\n",
      "Train_F1 score: 0.43131885158346134\n",
      "Train_Recall: 0.5104825949367089\n",
      "Train_Precision Score: 0.5104825949367089\n",
      "............................................\n",
      "model performance for test dataset\n",
      "Test Accuracy: 0.5158570855078282\n",
      "Test_F1 score: 0.43734260653809753\n",
      "Test_Recall: 0.5158570855078282\n",
      "Test_Precision Score: 0.5158570855078282\n"
     ]
    }
   ],
   "source": [
    "#10 RidgeClassifier\n",
    "\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier, RidgeClassifierCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "ridge = RidgeClassifier(alpha = 0.5, fit_intercept = True, normalize = False, \n",
    "                            copy_X = True, max_iter = None, tol = 0.001, class_weight = None, \n",
    "                            solver = \"auto\", random_state = 42)\n",
    "\n",
    "ridge.fit(x_train, y_train)\n",
    "\n",
    "y_pred_train = ridge.predict(x_train)\n",
    "y_pred_test = ridge.predict(x_test)\n",
    "\n",
    "# Calculate and store accuracy scores\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    " # Calculate and store F1 scores\n",
    "train_f1 = f1_score(y_train, y_pred_train, average='weighted') \n",
    "test_f1 = f1_score(y_test, y_pred_test, average='weighted') \n",
    "\n",
    "# Calculate and store recall scores\n",
    "train_recall = recall_score(y_train, y_pred_train, average='weighted')  # Use 'weighted' for multiclass problems\n",
    "test_recall = recall_score(y_test, y_pred_test, average='weighted')      # Use 'weighted' for multiclass problems\n",
    "\n",
    "# Calculate and store Precision scores\n",
    "train_precision = precision_score(y_train, y_pred_train, average='weighted') \n",
    "test_precision = precision_score(y_test, y_pred_test, average='weighted') \n",
    "\n",
    "\n",
    "print('model performance for training set')\n",
    "\n",
    "print(\"Train Accuracy: \"+str(accuracy_score(y_train, y_pred_train)))\n",
    "print ('Train_F1 score:', (f1_score(y_train, y_pred_train, average='weighted')))\n",
    "print ('Train_Recall:', (recall_score(y_train, y_pred_train, average='weighted')))\n",
    "print('Train_Precision Score:',(precision_score(y_train,  y_pred_train, pos_label='positive', average='micro')))\n",
    "\n",
    "print('............................................')\n",
    "\n",
    "\n",
    "print('model performance for test dataset')\n",
    "\n",
    "print(\"Test Accuracy: \"+str(accuracy_score(y_test, y_pred_test)))\n",
    "print ('Test_F1 score:', (f1_score(y_test, y_pred_test, average='weighted')))\n",
    "print ('Test_Recall:', (recall_score(y_test, y_pred_test, average='weighted')))\n",
    "print('Test_Precision Score:',(precision_score(y_test,  y_pred_test, pos_label='positive', average='micro')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "2334abf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model performance for training set\n",
      "Train Accuracy: 0.4990110759493671\n",
      "Train_F1 score: 0.45065031613000506\n",
      "Train_Recall: 0.4990110759493671\n",
      "Train_Precision Score: 0.4990110759493671\n",
      "............................................\n",
      "model performance for test dataset\n",
      "Test Accuracy: 0.513849859494179\n",
      "Test_F1 score: 0.4701838160896297\n",
      "Test_Recall: 0.513849859494179\n",
      "Test_Precision Score: 0.513849859494179\n"
     ]
    }
   ],
   "source": [
    "#11 logistic regression\n",
    "\n",
    "\n",
    "# train a logistic regression model on the training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(solver='liblinear')\n",
    "\n",
    "logreg.fit(x_train, y_train)\n",
    "\n",
    "# make class predictions for the testing set\n",
    "y_pred_test = logreg.predict(x_test)\n",
    "y_pred_train = logreg.predict(x_train)\n",
    "\n",
    "\n",
    "# Calculate and store accuracy scores\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    " # Calculate and store F1 scores\n",
    "train_f1 = f1_score(y_train, y_pred_train, average='weighted') \n",
    "test_f1 = f1_score(y_test, y_pred_test, average='weighted') \n",
    "\n",
    "# Calculate and store recall scores\n",
    "train_recall = recall_score(y_train, y_pred_train, average='weighted')  # Use 'weighted' for multiclass problems\n",
    "test_recall = recall_score(y_test, y_pred_test, average='weighted')      # Use 'weighted' for multiclass problems\n",
    "\n",
    "# Calculate and store Precision scores\n",
    "train_precision = precision_score(y_train, y_pred_train, average='weighted') \n",
    "test_precision = precision_score(y_test, y_pred_test, average='weighted') \n",
    "\n",
    "\n",
    "print('model performance for training set')\n",
    "\n",
    "print(\"Train Accuracy: \"+str(accuracy_score(y_train, y_pred_train)))\n",
    "print ('Train_F1 score:', (f1_score(y_train, y_pred_train, average='weighted')))\n",
    "print ('Train_Recall:', (recall_score(y_train, y_pred_train, average='weighted')))\n",
    "print('Train_Precision Score:',(precision_score(y_train,  y_pred_train, pos_label='positive', average='micro')))\n",
    "\n",
    "print('............................................')\n",
    "\n",
    "\n",
    "print('model performance for test dataset')\n",
    "\n",
    "print(\"Test Accuracy: \"+str(accuracy_score(y_test, y_pred_test)))\n",
    "print ('Test_F1 score:', (f1_score(y_test, y_pred_test, average='weighted')))\n",
    "print ('Test_Recall:', (recall_score(y_test, y_pred_test, average='weighted')))\n",
    "print('Test_Precision Score:',(precision_score(y_test,  y_pred_test, pos_label='positive', average='micro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "d2f16019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model performance for training set\n",
      "Train Accuracy: 0.5945411392405063\n",
      "Train_F1 score: 0.5834711409368893\n",
      "Train_Recall: 0.5945411392405063\n",
      "Train_Precision Score: 0.5945411392405063\n",
      "............................................\n",
      "model performance for test dataset\n",
      "Test Accuracy: 0.4588518667201927\n",
      "Test_F1 score: 0.44755891495841355\n",
      "Test_Recall: 0.4588518667201927\n",
      "Test_Precision Score: 0.4588518667201927\n"
     ]
    }
   ],
   "source": [
    "#12 KNeighborsClassifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "\n",
    "knn.fit(x_train, y_train)\n",
    "\n",
    "y_pred_test = knn.predict(x_test)\n",
    "y_pred_train = knn.predict(x_train)\n",
    "\n",
    "# Calculate and store accuracy scores\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    " # Calculate and store F1 scores\n",
    "train_f1 = f1_score(y_train, y_pred_train, average='weighted') \n",
    "test_f1 = f1_score(y_test, y_pred_test, average='weighted') \n",
    "\n",
    "# Calculate and store recall scores\n",
    "train_recall = recall_score(y_train, y_pred_train, average='weighted')  # Use 'weighted' for multiclass problems\n",
    "test_recall = recall_score(y_test, y_pred_test, average='weighted')      # Use 'weighted' for multiclass problems\n",
    "\n",
    "# Calculate and store Precision scores\n",
    "train_precision = precision_score(y_train, y_pred_train, average='weighted') \n",
    "test_precision = precision_score(y_test, y_pred_test, average='weighted') \n",
    "\n",
    "\n",
    "print('model performance for training set')\n",
    "\n",
    "print(\"Train Accuracy: \"+str(accuracy_score(y_train, y_pred_train)))\n",
    "print ('Train_F1 score:', (f1_score(y_train, y_pred_train, average='weighted')))\n",
    "print ('Train_Recall:', (recall_score(y_train, y_pred_train, average='weighted')))\n",
    "print('Train_Precision Score:',(precision_score(y_train,  y_pred_train, pos_label='positive', average='micro')))\n",
    "\n",
    "print('............................................')\n",
    "\n",
    "\n",
    "print('model performance for test dataset')\n",
    "\n",
    "print(\"Test Accuracy: \"+str(accuracy_score(y_test, y_pred_test)))\n",
    "print ('Test_F1 score:', (f1_score(y_test, y_pred_test, average='weighted')))\n",
    "print ('Test_Recall:', (recall_score(y_test, y_pred_test, average='weighted')))\n",
    "print('Test_Precision Score:',(precision_score(y_test,  y_pred_test, pos_label='positive', average='micro')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "94fe717b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model performance for training set\n",
      "Train Accuracy: 0.28441455696202533\n",
      "Train_F1 score: 0.1738801715088415\n",
      "Train_Recall: 0.28441455696202533\n",
      "Train_Precision Score: 0.28441455696202533\n",
      "............................................\n",
      "model performance for test dataset\n",
      "Test Accuracy: 0.2854275391409073\n",
      "Test_F1 score: 0.17476078831981892\n",
      "Test_Recall: 0.2854275391409073\n",
      "Test_Precision Score: 0.2854275391409073\n"
     ]
    }
   ],
   "source": [
    "#13 AdaBoostClassifier\n",
    "\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ABC = AdaBoostClassifier()\n",
    "\n",
    "ABC.fit(x_train, y_train)\n",
    "\n",
    "y_pred_test = ABC.predict(x_test)\n",
    "y_pred_train = ABC.predict(x_train)\n",
    "\n",
    "# Calculate and store accuracy scores\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    " # Calculate and store F1 scores\n",
    "train_f1 = f1_score(y_train, y_pred_train, average='weighted') \n",
    "test_f1 = f1_score(y_test, y_pred_test, average='weighted') \n",
    "\n",
    "# Calculate and store recall scores\n",
    "train_recall = recall_score(y_train, y_pred_train, average='weighted')  # Use 'weighted' for multiclass problems\n",
    "test_recall = recall_score(y_test, y_pred_test, average='weighted')      # Use 'weighted' for multiclass problems\n",
    "\n",
    "# Calculate and store Precision scores\n",
    "train_precision = precision_score(y_train, y_pred_train, average='weighted') \n",
    "test_precision = precision_score(y_test, y_pred_test, average='weighted') \n",
    "\n",
    "print('model performance for training set')\n",
    "\n",
    "print(\"Train Accuracy: \"+str(accuracy_score(y_train, y_pred_train)))\n",
    "print ('Train_F1 score:', (f1_score(y_train, y_pred_train, average='weighted')))\n",
    "print ('Train_Recall:', (recall_score(y_train, y_pred_train, average='weighted')))\n",
    "print('Train_Precision Score:',(precision_score(y_train,  y_pred_train, pos_label='positive', average='micro')))\n",
    "\n",
    "print('............................................')\n",
    "\n",
    "\n",
    "print('model performance for test dataset')\n",
    "\n",
    "print(\"Test Accuracy: \"+str(accuracy_score(y_test, y_pred_test)))\n",
    "print ('Test_F1 score:', (f1_score(y_test, y_pred_test, average='weighted')))\n",
    "print ('Test_Recall:', (recall_score(y_test, y_pred_test, average='weighted')))\n",
    "print('Test_Precision Score:',(precision_score(y_test,  y_pred_test, pos_label='positive', average='micro')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "dbb9c500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model performance for training set\n",
      "Train Accuracy: 0.3659018987341772\n",
      "Train_F1 score: 0.2903579988658566\n",
      "Train_Recall: 0.3659018987341772\n",
      "Train_Precision Score: 0.3659018987341772\n",
      "............................................\n",
      "model performance for test dataset\n",
      "Test Accuracy: 0.37173825772782015\n",
      "Test_F1 score: 0.2990529150642882\n",
      "Test_Recall: 0.37173825772782015\n",
      "Test_Precision Score: 0.37173825772782015\n"
     ]
    }
   ],
   "source": [
    "#14 SVC\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "svc=SVC() \n",
    "\n",
    "svc.fit(x_train, y_train)\n",
    "\n",
    "y_pred_test = svc.predict(x_test)\n",
    "y_pred_train = svc.predict(x_train)\n",
    "\n",
    "# Calculate and store accuracy scores\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    " # Calculate and store F1 scores\n",
    "train_f1 = f1_score(y_train, y_pred_train, average='weighted') \n",
    "test_f1 = f1_score(y_test, y_pred_test, average='weighted') \n",
    "\n",
    "# Calculate and store recall scores\n",
    "train_recall = recall_score(y_train, y_pred_train, average='weighted')  # Use 'weighted' for multiclass problems\n",
    "test_recall = recall_score(y_test, y_pred_test, average='weighted')      # Use 'weighted' for multiclass problems\n",
    "\n",
    "# Calculate and store Precision scores\n",
    "train_precision = precision_score(y_train, y_pred_train, average='weighted') \n",
    "test_precision = precision_score(y_test, y_pred_test, average='weighted') \n",
    "\n",
    "\n",
    "print('model performance for training set')\n",
    "\n",
    "print(\"Train Accuracy: \"+str(accuracy_score(y_train, y_pred_train)))\n",
    "print ('Train_F1 score:', (f1_score(y_train, y_pred_train, average='weighted')))\n",
    "print ('Train_Recall:', (recall_score(y_train, y_pred_train, average='weighted')))\n",
    "print('Train_Precision Score:',(precision_score(y_train,  y_pred_train, pos_label='positive', average='micro')))\n",
    "\n",
    "print('............................................')\n",
    "\n",
    "\n",
    "print('model performance for test dataset')\n",
    "\n",
    "print(\"Test Accuracy: \"+str(accuracy_score(y_test, y_pred_test)))\n",
    "print ('Test_F1 score:', (f1_score(y_test, y_pred_test, average='weighted')))\n",
    "print ('Test_Recall:', (recall_score(y_test, y_pred_test, average='weighted')))\n",
    "print('Test_Precision Score:',(precision_score(y_test,  y_pred_test, pos_label='positive', average='micro')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa03c53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
